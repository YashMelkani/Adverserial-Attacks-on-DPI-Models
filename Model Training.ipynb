{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748c0907-7982-4c4d-a26c-b19d66d8413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1cb37a-7f74-4d7d-96ea-aaa25475e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "  # 'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, samp_idxs, base_path = \"./data/\"):\n",
    "        # 'Initialization'\n",
    "        self.base_path = base_path\n",
    "        self.samp_idxs = samp_idxs # a list of the indexs of the samples to use \n",
    "        \n",
    "    def __len__(self):\n",
    "        # 'Denotes the total number of samples'\n",
    "        return len(self.samp_idxs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i = self.samp_idxs[index]\n",
    "        df = np.load(f\"{self.base_path}dens{i:05d}.npy\") # density field\n",
    "        labels = np.load(f\"{self.base_path}labels.npy\")\n",
    "        label = labels[i, :]\n",
    "        \n",
    "        df = np.expand_dims(df, 0)\n",
    "        return df, label\n",
    "\n",
    "class QuickDataset(Dataset):\n",
    "    def __init__(self, samp_idxs, base_path = \"./data/\"):\n",
    "        # 'Initialization'\n",
    "        self.base_path = base_path\n",
    "        self.samp_idxs = samp_idxs # a list of the indexs of the samples to use \n",
    "\n",
    "        self.dfs = np.zeros((len(samp_idxs), 64, 64), dtype=np.float32)\n",
    "        self.labels = np.zeros((len(samp_idxs), 2))\n",
    "        \n",
    "        all_labels = np.load(f\"{self.base_path}labels.npy\")\n",
    "        \n",
    "        for i in tqdm(range(len(samp_idxs))):\n",
    "            data_idx = samp_idxs[i]\n",
    "            self.dfs[i, :, :] = np.load(f\"{self.base_path}dens{data_idx:05d}.npy\")\n",
    "            self.labels[i, :] = all_labels[data_idx, :]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 'Denotes the total number of samples'\n",
    "        return len(self.samp_idxs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df = self.dfs[index, :, :]\n",
    "        label = self.labels[index, :]\n",
    "        \n",
    "        df = np.expand_dims(df, 0)\n",
    "        return df, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12dc47a-26f1-4e3d-81b6-334d40595e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [01:34<00:00, 74.35it/s] \n",
      "100%|██████████| 1500/1500 [00:17<00:00, 83.38it/s]\n",
      "100%|██████████| 1500/1500 [00:20<00:00, 74.09it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bs = 64\n",
    "\n",
    "train_idxs = np.arange(0, 7000)\n",
    "val_idxs = np.arange(7000, 8500)\n",
    "test_idxs = np.arange(8500, 10000)\n",
    "\n",
    "# trainset = Dataset(train_idxs)\n",
    "# testset = Dataset(test_idxs)\n",
    "\n",
    "trainset = QuickDataset(train_idxs)\n",
    "valset = QuickDataset(val_idxs)\n",
    "testset = QuickDataset(test_idxs)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=bs, shuffle=True, pin_memory = True)\n",
    "val_loader = DataLoader(valset, batch_size=bs, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=bs, shuffle=True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ba23d1-f8c0-49da-8205-d2a1d05928f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses some parts from https://github.com/tonyduan/mixture-density-network/blob/master/src/blocks.py\n",
    "class MyMDM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn_layers = Sequential( #given an input shape of 64x64\n",
    "            nn.Conv2d(1, 4, (3,3), padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 8, (3,3), padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4),\n",
    "            nn.Conv2d(8, 16, (3,3), padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (3,3), padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4),\n",
    "        )\n",
    "        \n",
    "        self.dim_in = 4*4*32 # size of output from cnn_layers\n",
    "        self.dim_out = 2 #number of parameters in output (A_s_1e9 and Omega_m)\n",
    "        self.n_components = 1 #number of components in mixture model(each parameter explained by single gaussian)\n",
    "        num_sigma_channels = self.dim_out * self.n_components # because we assume diagonal covariance  \n",
    "        \n",
    "        self.pi_network = nn.Sequential(\n",
    "            nn.Linear(self.dim_in, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.n_components),\n",
    "        )\n",
    "        self.normal_network = nn.Sequential(\n",
    "            nn.Linear(self.dim_in, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.dim_out * self.n_components + num_sigma_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        #\n",
    "        # Returns\n",
    "        # -------\n",
    "        # log_pi: (bsz, n_components)\n",
    "        # mu: (bsz, n_components, dim_out)\n",
    "        # sigma: (bsz, n_components, dim_out)\n",
    "        #\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        log_pi = torch.log_softmax(self.pi_network(x), dim=-1)\n",
    "        normal_params = self.normal_network(x)\n",
    "        mu = normal_params[..., :self.dim_out * self.n_components]\n",
    "        sigma = normal_params[..., self.dim_out * self.n_components:]\n",
    "        sigma = torch.exp(sigma + eps) # <-- this will need to change if we are not assuming diagonal covariance\n",
    "        mu = mu.reshape(-1, self.n_components, self.dim_out)\n",
    "        sigma = sigma.reshape(-1, self.n_components, self.dim_out)\n",
    "        return log_pi, mu, sigma\n",
    "\n",
    "    def loss(self, y, log_pi, mu, sigma):\n",
    "        # print(y[0, :])\n",
    "        # print(log_pi[0,:])\n",
    "        # print(mu[0,:, :])\n",
    "        # print(sigma[0,:,:])\n",
    "        \n",
    "        z_score = (y.unsqueeze(1) - mu) / sigma\n",
    "        normal_loglik = (\n",
    "            -0.5 * torch.einsum(\"bij,bij->bi\", z_score, z_score)\n",
    "            -torch.sum(torch.log(sigma), dim=-1)\n",
    "        )\n",
    "        loglik = torch.logsumexp(log_pi + normal_loglik, dim=-1)\n",
    "        return -loglik\n",
    "\n",
    "    def sample(self, x):\n",
    "        log_pi, mu, sigma = self.forward(x)\n",
    "        cum_pi = torch.cumsum(torch.exp(log_pi), dim=-1)\n",
    "        rvs = torch.rand(len(x), 1).to(x)\n",
    "        rand_pi = torch.searchsorted(cum_pi, rvs)\n",
    "        rand_normal = torch.randn_like(mu) * sigma + mu\n",
    "        samples = torch.gather(rand_normal, index=rand_pi.unsqueeze(-1), dim=1).squeeze(dim=1)\n",
    "        return samples \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5336383-905a-4968-9d07-fd8f6b0e7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    epochs = 750\n",
    "    optim = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, verbose = False)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 50)\n",
    "    \n",
    "    train_ep_losses = [] #stores the sum of train_loss\n",
    "    val_ep_losses = [] #stores the sum of val_loss \n",
    "    val_epochs = [] # stores the epochs at which validation set is evaluated\n",
    "    min_val_loss = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_ep_loss = []\n",
    "        val_ep_loss = []\n",
    "        model.train()\n",
    "\n",
    "        for b, batch in enumerate(train_loader):\n",
    "\n",
    "            df, label = batch\n",
    "\n",
    "            df = df.to(device) / 1000 #some normalization\n",
    "            label = label.to(device)\n",
    "            label[:, 0] = label[:, 0]/10 #Bring A_s_1e9 range from [0.5, 5.5] to [0.05, 0.55]      \n",
    "\n",
    "            pred = model(df)\n",
    "            loss = model.loss(label, *pred).mean()\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optim.step()\n",
    "            \n",
    "            train_ep_loss.append(loss.item())\n",
    "            \n",
    "        train_ep_losses.append(sum(train_ep_loss)/len(train_ep_loss))\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for b, batch in enumerate(val_loader):\n",
    "\n",
    "                df, label = batch\n",
    "                \n",
    "                df = df.to(device) / 1000 #some normalization\n",
    "                label = label.to(device)\n",
    "                label[:, 0] = label[:, 0]/10 #Bring A_s_1e9 range from [0.5, 5.5] to [0.05, 0.55]      \n",
    "\n",
    "                pred = model(df)\n",
    "                loss = model.loss(label, *pred).mean()\n",
    "                \n",
    "                val_ep_loss.append(loss.item())\n",
    "                \n",
    "            val_ep_losses.append(sum(val_ep_loss)/len(val_ep_loss))\n",
    "            val_epochs.append(epoch)\n",
    "            scheduler.step(val_ep_losses[-1])\n",
    "        \n",
    "        if val_ep_losses[-1] < min_val_loss:\n",
    "           \n",
    "            min_val_loss = val_ep_losses[-1]\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optim.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'train_loss': train_ep_loss,\n",
    "                    'val_loss': val_ep_loss\n",
    "                    }, f\"./model_ckpt.tar\")\n",
    "        \n",
    "        plt.plot(train_ep_losses, label = \"Train Loss\")\n",
    "        plt.plot(val_epochs, val_ep_losses, label = \"Val Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"./training_val_loss.png\")\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e903cd84-40b2-43b1-9319-fe7a7e1989c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num trainable params: 171141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [12:31<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = MyMDM()\n",
    "\n",
    "print(\"Num trainable params:\",  sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        y = m.in_features\n",
    "        m.weight.data.normal_(0.0,1/np.sqrt(y))\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "model.apply(weights_init_normal)\n",
    "\n",
    "model = model.to(device = device)\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb0c7cb-ec2e-4fcf-8a34-7ee3f38b9168",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026fec54-00a2-43c8-8003-10eea0a155da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyMDM()\n",
    "model_cp = torch.load(\"./model_ckpt.tar\")\n",
    "model.load_state_dict(model_cp[\"model_state_dict\"])\n",
    "model.eval()\n",
    "model = model.to(device = device)\n",
    "\n",
    "pred = None\n",
    "for b, batch in enumerate(test_loader):\n",
    "\n",
    "    df, label = batch\n",
    "\n",
    "    df = df.to(device) / 1000 #some normalization\n",
    "    label[:, 0] = label[:, 0]/10 #Bring A_s_1e9 range from [0.5, 5.5] to [0.05, 0.55]    \n",
    "    \n",
    "    pred = model(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7db88fa-22a4-4146-9aae-0f69758d5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = pred[1].detach().cpu().numpy()[:,0,:]\n",
    "sigmas = pred[2].detach().cpu().numpy()[:,0,:]\n",
    "\n",
    "labels = label.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979f4aea-d0f2-40fb-881f-35afff3500e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega_m | Label: 0.35000000000000003 | Pred: 0.37367725 +/- 0.061160143\n",
      "Omega_m | Label: 0.45 | Pred: 0.47929844 +/- 0.08339021\n",
      "Omega_m | Label: 0.25 | Pred: 0.29973027 +/- 0.06623718\n",
      "Omega_m | Label: 0.5 | Pred: 0.4699641 +/- 0.04503933\n",
      "Omega_m | Label: 0.25 | Pred: 0.24811687 +/- 0.069735296\n",
      "Omega_m | Label: 0.2 | Pred: 0.33428103 +/- 0.07652944\n",
      "Omega_m | Label: 0.35000000000000003 | Pred: 0.38484484 +/- 0.070030704\n",
      "Omega_m | Label: 0.2 | Pred: 0.16515684 +/- 0.036609013\n",
      "Omega_m | Label: 0.05 | Pred: 0.053703614 +/- 0.003204\n",
      "Omega_m | Label: 0.35000000000000003 | Pred: 0.37044168 +/- 0.10709389\n",
      "Omega_m | Label: 0.15000000000000002 | Pred: 0.22771005 +/- 0.052512005\n",
      "Omega_m | Label: 0.3 | Pred: 0.19820632 +/- 0.04593917\n",
      "Omega_m | Label: 0.25 | Pred: 0.29672006 +/- 0.07785397\n",
      "Omega_m | Label: 0.5 | Pred: 0.33930328 +/- 0.06986257\n",
      "Omega_m | Label: 0.15000000000000002 | Pred: 0.13587587 +/- 0.02919797\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    # print(\"A_s_1e8 | Label:\", labels[i,0], \"| Pred:\", mus[i,0], \"+/-\", sigmas[i,0])\n",
    "    print(\"Omega_m | Label:\", labels[i,1], \"| Pred:\", mus[i,1], \"+/-\", sigmas[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58f96244-ce48-408f-9fe2-cec1d4e72d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00921f8f-6bd3-449b-9c8b-5c3ed56a41c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94880a30-2875-4c57-8fd1-0b1f6552a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 64, 64])\n",
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "for b, batch in enumerate(train_loader):\n",
    "\n",
    "    df, label = batch\n",
    "    print(df.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f57f07-b75d-4529-b577-37fd2f63b544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:horowitz]",
   "language": "python",
   "name": "conda-env-horowitz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
